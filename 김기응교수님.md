1,4,5,7

아래의 논문들과, 내 연구주제는 어떤 부분에서 관련이 있을까? 내가 하고자 하는 연구는 "나의 연구:" 바로 밑에 표기했어.
또 기계공학과 관련이 있으면 좋을 것 같아.

나의 연구:
다수의 uav의 비행계획과 외부환경정보를 기반으로 행동트리를 생성하고, 이상징후 탐지 성능을 개선하는 방향으로 제어시스템을 학습시키는 것

# 논문1:
# 연속적인 행동 공간이 있는 환경에서 강화 학습(RL)을 위한 결정론적 목표 정책의 오프 정책 평가(OPE)를 고려합니다. OPE에 중요도 샘플링을 사용하는 것이 일반적이지만 행동 정책이 대상 정책에서 크게 벗어날 때 높은 변동이 발생합니다. 이 문제를 해결하기 위해 OPE에 대한 일부 최근 연구에서는 중요도 리샘플링을 사용한 샘플 내 학습을 제안했습니다. 그러나 이러한 접근 방식은 연속 행동 공간에 대한 결정론적 대상 정책에는 적용되지 않습니다. 이러한 한계를 해결하기 위해 우리는 커널을 사용하여 결정론적 대상 정책을 완화하고 행동 값 함수의 추정된 시간차 업데이트 벡터의 전체 평균 제곱 오차를 최소화하는 커널 메트릭을 학습할 것을 제안합니다. 여기서 행동 값 함수는 정책에 사용됩니다. 평가. 우리는 이러한 완화로 인한 추정 오차의 편향과 분산을 도출하고 최적의 커널 메트릭에 대한 분석 솔루션을 제공합니다. 다양한 테스트 도메인을 사용한 실증적 연구에서 최적화된 메트릭을 사용하는 커널을 사용하는 샘플 내 학습을 갖춘 OPE가 다른 기준보다 훨씬 향상된 정확도를 달성한다는 것을 보여줍니다.

# 논문2:
# 편집된 언어 모델은 다양한 자연어 클래스에 더할 수 있는 잠재력을 갖고 있고, 사전 훈련된 일부의 특정 소수를 특정 하위 작업에 업데이트하는 기존의 파인튜닝은 대용량의 컴퓨팅을 선호합니다. 모든 종류의 버섯을 업데이트하는 대신 기존의 수신기에 업데이트하는 것을 추가하여 학습하는 리튬이온 프롬프팅 방법론이 제공되고 있지만, 학습된 좀비가 있을 때 해석하는 것이 불가능하거나 불가능하지는 않지만 가변적인 지점이 있었습니다. 본 논문에서는 이러한 점을 해결하기 위해, 강화학습 범위의 동작 모사 학습을 기반으로 사람에게 그럼에도 불구하고 예제 프롬 프트를 활용하는 해석을 최적화하는 것을 제안합니다.

# 논문3:
# 전체 모델 매개변수를 미세 조정하는 대신 사전 훈련된 대규모 언어 모델을 다운스트림 작업에 적용하도록 프롬프트를 최적화하는 프롬프트 조정은 다중 작업 전이 학습에서 프롬프트를 훈련할 때 특히 효과적인 것으로 나타났습니다. 환경. 이러한 방법에는 일반적으로 각 소스 작업에 대한 프롬프트를 개별적으로 교육한 다음 이를 집계하여 대상 작업에 대한 프롬프트 초기화를 제공하는 작업이 포함됩니다. 그러나 이 접근 방식은 일부 소스 작업이 서로 부정적 또는 긍정적으로 간섭할 수 있다는 사실을 비판적으로 무시합니다. 우리는 훈련 소스 프롬프트를 통해 소스 작업에서 지식을 추출할 때 대상 작업으로 더 나은 전환을 위해 소스 작업 간의 이러한 상관 관계를 고려해야 한다고 주장합니다. 이를 위해 우리는 소스 작업 전반에 걸쳐 프롬프트의 사후 분포를 다루는 베이지안 접근 방식을 제안합니다. Stein Variational Gradient Descent를 활용하여 후방 샘플에 해당하는 대표 소스 프롬프트를 얻은 다음 이를 집계하여 초기 대상 프롬프트를 구성합니다. 우리는 베이지안 다중 작업 전이 학습 접근 방식이 여러 설정에서 최첨단 방법보다 성능이 뛰어난 표준 벤치마크 NLP 작업에 대한 광범위한 실험 결과를 보여줍니다. 또한, 우리의 접근 방식은 프롬프트 자체 외에는 보조 모델이 필요하지 않아 높은 수준의 매개변수 효율성을 달성합니다.

# 논문4:
# 부분적으로 관찰 가능한 환경의 경우 관찰 기록을 통한 모방 학습(ILOH)은 제어 관련 정보가 전문가 작업을 모방하기 위해 관찰 기록에 충분히 캡처되었다고 가정합니다. 에이전트가 환경과의 상호 작용 없이 모방하는 방법을 배워야 하는 오프라인 환경에서 행동 복제(BC)는 모방 학습을 위한 간단하면서도 효과적인 방법인 것으로 나타났습니다. 그러나 과거의 타임스텝에서 수행된 행위에 대한 정보가 관찰 이력에 유출되면 BC를 통한 ILOH는 자신의 과거 행위를 모방하는 경우가 많다. 이 문서에서는 PALR(Past Action Leakage Regularization)이라는 BC에 대한 원칙적인 정규화를 제안하여 이러한 치명적인 실패를 해결합니다. 우리 접근 방식의 주요 아이디어는 조건부 독립성의 고전적 개념을 활용하여 누출을 완화하는 것입니다. 우리는 프레임워크의 다양한 인스턴스를 조건부 독립 메트릭과 해당 추정자의 자연스러운 선택과 비교합니다. 우리의 비교 결과는 조건부 독립 메트릭에 대해 특정 커널 기반 추정기의 사용을 옹호합니다. 우리는 정규화 방법의 효율성을 평가하기 위해 벤치마크 데이터 세트에 대한 광범위한 실험을 수행합니다. 실험 결과는 우리의 방법이 이전 관련 접근 방식보다 훨씬 뛰어나며 과거 행동 정보가 관찰 기록으로 유출될 때 전문가 행동을 성공적으로 모방할 수 있는 잠재력을 강조한다는 것을 보여줍니다.

논문5:
오프라인 강화 학습(RL)의 주요 과제 중 하나는 데이터 수집 정책에서 벗어나 학습된 정책으로 인해 발생하는 분포 변화입니다. 이는 정책 개선 중에 배포 외(OOD) 조치를 피함으로써 해결되는 경우가 많습니다. OOD 조치가 있으면 상당한 성능 저하가 발생할 수 있기 때문입니다. 이 문제는 에이전트 수에 따라 공동 작업 공간이 기하급수적으로 증가하기 때문에 오프라인 MARL(Multi-Agent RL) 설정에서 증폭됩니다. 이러한 차원의 저주를 피하기 위해 기존 MARL 방법은 가치 분해 방법 또는 개별 에이전트의 완전히 분산된 교육을 채택합니다. . 그러나 표준 보수주의 원칙과 결합하더라도 이러한 방법은 여전히 ​​오프라인 MARL에서 OOD 공동 조치를 선택할 수 있습니다. 이를 위해 고정 분포 최적화를 기반으로 개별 에이전트의 중앙 집중식 훈련을 대안적으로 수행하는 오프라인 MARL 알고리즘인 AlberDICE를 소개합니다. AlberDICE는 OOD 공동 작업 선택을 효과적으로 피하면서 동시에 한 에이전트의 최상의 응답을 계산하여 MARL의 기하급수적 복잡성을 우회합니다. 이론적으로 우리는 교대 최적화 절차가 Nash 정책으로 수렴된다는 것을 보여줍니다. 실험에서 우리는 AlberDICE가 MARL 벤치마크의 표준 제품군에서 기본 알고리즘보다 훨씬 뛰어난 성능을 발휘함을 보여줍니다.

# 논문6:
# 대화 시스템 기술 공모전(DSTC)을 통해 대화 시스템이 눈부시게 발전했지만, 음성 인터페이스를 갖춘 강력한 작업 중심 대화 시스템을 구축하는 것은 여전히 ​​주요 과제 중 하나입니다. 서면 말뭉치가 풍부한 데이터 세트가 있는 반면 음성 대화가 있는 데이터 세트는 매우 드물기 때문에 텍스트 기반 대화 시스템에 대한 대부분의 진전이 이루어졌습니다. 그러나 시리(Siri), 알렉사(Alexa)와 같은 음성 비서 시스템에서 볼 수 있듯이 음성 대화로 성공을 옮기는 것이 실질적으로 중요합니다. 이 문서에서는 DSTC11의 음성 인식 대화 시스템 기술 챌린지 트랙에 참여한 매우 성공적인 모델을 구축하기 위한 엔지니어링 노력에 대해 설명합니다. 우리 모델은 세 가지 주요 모듈로 구성됩니다. (1) 음성과 텍스트 발화 사이의 격차를 해소하기 위한 자동 음성 인식 오류 수정, (2) 슬롯 설명을 사용하여 슬롯과 값을 추정하는 텍스트 기반 대화 시스템(D3ST), (3) 추정된 슬롯 값의 오류를 복구하기 위한 후처리. 우리의 실험은 음성 대화 말뭉치를 위한 텍스트 기반 대화 상태 추적기를 적용하기 위해 명시적 자동 음성 인식 오류 수정 모듈, 후처리 및 데이터 증대를 사용하는 것이 중요하다는 것을 보여줍니다.

# 논문7:
# MVRL(Multi-View Reinforcement Learning)은 다양한 소스의 다중 뷰 관찰을 통해 에이전트에 대한 최적의 제어를 찾으려고 합니다. 다중 뷰 데이터에서 잠재 표현을 추출하는 것을 목표로 하는 다중 뷰 학습의 최근 발전에도 불구하고 이를 제어 작업에 적용하는 것은 간단하지 않습니다. 특히 관찰이 시간적으로 서로 의존하는 경우에는 더욱 그렇습니다. 뷰의 하위 집합에 대한 관측치가 간헐적으로 누락되는 경우 문제는 훨씬 더 어려울 수 있습니다. 본 논문에서는 다중 뷰 관찰 시퀀스에서 기본 상태 공간 모델을 캡처하는 정보 이론적 접근 방식인 Fuse2Control(F2C)을 소개합니다. 우리는 다양한 제어 작업에서 광범위한 실험을 수행하여 우리의 방법이 임의의 누락된 뷰 시나리오에 대한 견고성을 유지하면서 뷰 수에 따라 선형적으로 확장되는 여러 뷰에서 작업 관련 정보를 집계하는 데 매우 효과적이라는 것을 보여줍니다.

# 논문8:
# 자동차의 잔존가치(RV)는 미래 특정 시점의 예상 가치를 나타냅니다. 이는 모든 자동차 금융 상품의 핵심 구성 요소로, 신용 한도와 리스 요금을 결정하는 데 사용됩니다. 이처럼 자동차 금융산업에서는 RV에 대한 정확한 예측이 매우 중요합니다. 과도한 예측으로 인해 수익 손실이 발생하거나, 과소 예측으로 인해 금융상품이 무능해질 위험이 있기 때문입니다. 대량의 중고차 판매 데이터를 대상으로 기계 학습 모델을 훈련하는 것에 대한 이전 연구가 많이 있지만, 규정 준수(즉, 기능의 하위 집합에 대한 출력의 단조성)와 같은 실제 운영 요구 사항에 대처해야 했습니다. ) 및 보이지 않는 입력(즉, 새롭고 희귀한 자동차 모델)에 대한 일반화입니다. 본 논문에서는 한국 최고의 자동차 금융 서비스 제공업체인 현대캐피탈서비스에서 이러한 현실적 과제에 어떻게 대처하고 비즈니스 가치를 창출했는지 설명합니다.


아래의 논문과, 내 연구주제는 어떤 부분에서 관련이 있을까? 내가 하고자 하는 연구는 "나의 연구:" 바로 밑에 표기했어.
또 기계공학과 관련이 있으면 좋을 것 같아.

나의 연구:
다수의 uav의 비행계획과 외부환경정보를 기반으로 행동트리를 생성하고, 이상징후 탐지 성능을 개선하는 방향으로 제어시스템을 학습시키는 것


논문:
오프라인 강화 학습(RL)의 주요 과제 중 하나는 데이터 수집 정책에서 벗어나 학습된 정책으로 인해 발생하는 분포 변화입니다. 이는 정책 개선 중에 배포 외(OOD) 조치를 피함으로써 해결되는 경우가 많습니다. OOD 조치가 있으면 상당한 성능 저하가 발생할 수 있기 때문입니다. 이 문제는 에이전트 수에 따라 공동 작업 공간이 기하급수적으로 증가하기 때문에 오프라인 MARL(Multi-Agent RL) 설정에서 증폭됩니다. 이러한 차원의 저주를 피하기 위해 기존 MARL 방법은 가치 분해 방법 또는 개별 에이전트의 완전히 분산된 교육을 채택합니다. . 그러나 표준 보수주의 원칙과 결합하더라도 이러한 방법은 여전히 ​​오프라인 MARL에서 OOD 공동 조치를 선택할 수 있습니다. 이를 위해 고정 분포 최적화를 기반으로 개별 에이전트의 중앙 집중식 훈련을 대안적으로 수행하는 오프라인 MARL 알고리즘인 AlberDICE를 소개합니다. AlberDICE는 OOD 공동 작업 선택을 효과적으로 피하면서 동시에 한 에이전트의 최상의 응답을 계산하여 MARL의 기하급수적 복잡성을 우회합니다. 이론적으로 우리는 교대 최적화 절차가 Nash 정책으로 수렴된다는 것을 보여줍니다. 실험에서 우리는 AlberDICE가 MARL 벤치마크의 표준 제품군에서 기본 알고리즘보다 훨씬 뛰어난 성능을 발휘함을 보여줍니다.